% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/fit_model.R
\name{fit_model}
\alias{fit_model}
\title{Fit a Random Forest with optional resampling, tuning, and progress bars}
\usage{
fit_model(
  x,
  y,
  rf_args = list(ntree = 500, mtry = NULL, nodesize = 5, sampsize = NULL),
  test = list(method = "none", k = 5, test_size = 0.3, seed = NULL, folds = NULL,
    iterations = 200, correction = FALSE),
  tune = list(enable = FALSE, search = "grid", grid = NULL, n_random = 20, seed = NULL),
  verbose = TRUE,
  list_test_models = TRUE
)
}
\arguments{
\item{x}{A data.frame of predictors (rows = samples, cols = features).}

\item{y}{A numeric vector of responses, \code{length(y) == nrow(x)}.}

\item{rf_args}{A named list of base Random Forest arguments used for all
fits (full-data fit and each resample). Elements:
\itemize{
\item \code{ntree} (integer, default \code{500}): number of trees.
\item \code{mtry} (integer or \code{NULL}): number of variables sampled at each split.
If \code{NULL}, a safe default \code{floor(sqrt(p))} is used.
\item \code{nodesize} (integer, default \code{5}): minimum terminal node size.
\item \code{sampsize} (integer, fraction in \verb{(0,1]}, or \code{NULL}):
sample size per tree. If a single fraction, it is multiplied by the
current training size and rounded. If \code{NULL}, the package default is used.
}}

\item{test}{A named list configuring evaluation:
\itemize{
\item \code{method} (character): one of \code{none}, \code{loocv}, \code{k-fold} (also accepts \code{kfold}/\verb{k fold}),
\code{split}, or \code{bootstrap}.
\item \code{k} (integer, default \code{5}): number of folds for K-fold CV.
\item \code{folds} (list or \code{NULL}): custom index list of test-fold indices.
If supplied, overrides \code{k}.
\item \code{test_size} (numeric in \verb{(0,1)}, default \code{0.3}): test fraction for
train/test split.
\item \code{iterations} (integer, default \code{200}): number of bootstrap resamples.
\item \code{correction} (logical, default \code{FALSE}): if \code{TRUE}, apply the
.632 correction to the RMSE for the bootstrap estimate.
\item \code{seed} (integer or \code{NULL}): RNG seed for reproducibility (folds,
split, random tuning).
}}

\item{tune}{A named list configuring hyperparameter search on the \emph{training}
subset for each fit:
\itemize{
\item \code{enable} (logical, default \code{FALSE}): turn tuning on/off.
\item \code{search} (character, default \code{grid}): \code{grid} or \code{random}.
\item \code{grid} (data.frame or \code{NULL}): explicit grid with columns
\code{mtry}, \code{ntree}, \code{nodesize}, \code{sampsize}. If \code{NULL}, a sensible
default grid is generated from \code{p = ncol(x)}.
\item \code{n_random} (integer, default \code{20}): number of random draws from
the grid when \code{search = "random"}.
\item \code{seed} (integer or \code{NULL}): RNG seed for the tuning subset draw order.
}}

\item{verbose}{Logical (default \code{TRUE}): show progress bars/messages for
tuning, LOOCV, K-fold, and bootstrap loops. Set \code{FALSE} to silence.}

\item{list_test_models}{Logical (default \code{TRUE}): if \code{TRUE}, save the
\emph{per-resample} fitted model objects as a list in \code{models_test}.
This can be large, especially for \code{bootstrap} with many iterations.}
}
\value{
A list with:
\describe{
\item{method}{\code{rf}}
\item{rf_args}{Final RF arguments used for the full-data fit (after tuning).}
\item{tune_table}{(data.frame or \code{NULL}) tuning results sorted by OOB MSE.}
\item{test}{Echo of \code{test} configuration (with resolved values).}
\item{model}{\code{randomForest} object fit on the full dataset (or train subset for \code{split}).}
\item{fitted_full}{Numeric vector of in-sample predictions from the full-data fit.}
\item{stats_train}{data.frame of training statistics (RMSE, Bias, \%RMSE, \%Bias, r, r2).}
\item{stats_test}{(data.frame or \code{NULL}) test-set or OOF statistics depending on method.}
\item{loocv_pred}{(numeric) LOOCV predictions (for \code{method = "loocv"}).}
\item{cv_pred}{(numeric) K-fold OOF predictions (for \code{method = "k-fold"}).}
\item{train_index,test_index}{(integer) indices for \code{method = "split"}.}
\item{pred_train,pred_test}{(numeric) predictions for train/test in \code{split}.}
\item{oob_pred}{(numeric) OOB mean prediction per observation in \code{bootstrap}.}
\item{models_test}{(list or \code{NULL}) models fitted per resample/fold/iteration when \code{list_test_models=TRUE}.}
}
}
\description{
\code{fit_model()} trains a Random Forest regression model and optionally
evaluates it via LOOCV, K-fold CV, a train/test split, or bootstrap OOB
estimation. It can also tune core RF hyperparameters and shows progress
bars for long-running loops.
}
\details{
Tuning minimizes OOB MSE for each candidate configuration on the current
training subset and then refits the model using the best settings.
When \code{test$method = "bootstrap"} and \code{correction = TRUE}, the .632 corrected
RMSE is reported while keeping other OOB statistics unchanged.
}
\section{Workflow}{

\enumerate{
\item Optionally tune RF hyperparameters on the \emph{current training subset}
(or on full data when \code{test$method = "none"}).
\item Always fit a model on the full dataset (\code{model} and \code{fitted_full}).
\item If a testing method is requested, refit on resampled training folds and
compute out-of-fold predictions to summarize generalization performance.
}
}

\section{Progress Bars}{

Uses \code{utils::txtProgressBar()}; disable with \code{verbose = FALSE}. The internal
helper is lightweight and has no external dependencies.
}

\examples{
set.seed(42)
n <- 200
x <- data.frame(NDVI = runif(n, 0.2, 0.9),
                EVI  = runif(n, 0.1, 0.8),
                NBR  = runif(n, -0.5, 0.9),
                SLP  = runif(n, 0, 30))
y <- with(x, 5 + 20*NDVI + 10*EVI^1.5 - 0.05*SLP + rnorm(n, 0, 2))

# Full-data fit (no resampling)
fit_none <- fit_model(x, y, rf_args = list(ntree = 400, mtry = 2))
fit_none$stats_train

# LOOCV
\dontshow{
  # Make it smaller for CRAN checks
  x <- x[1:20, ]
  y <- y[1:20]
}
fit_loocv <- fit_model(x, y, test = list(method = "loocv"))
fit_loocv$stats_test

# 5-fold CV
fit_k_fold <- fit_model(x, y, test = list(method = "k-fold", k = 5, seed = 42))
fit_k_fold$stats_test

# Train/Test split
fit_split <- fit_model(x, y, test = list(method = "split", test_size = 0.25, seed = 42))
fit_split$stats_test

# Bootstrap (with tuning and .632 correction)
ntree <- 400
iterations <- 300
\dontshow{
  # Smaller tree and iterations for checks
  ntree <- 50
  iterations <- 10
}
fit_boot <- fit_model(
  x, y,
  rf_args = list(ntree = ntree),
  test    = list(method = "bootstrap", iterations = iterations, correction = TRUE),
  tune    = list(enable = TRUE, search = "random", n_random = 12)
)

# K-fold CV with saved models
fit_k <- fit_model(x, y, test = list(method = "k-fold", k = 5, seed = 42), list_test_models = TRUE)
length(fit_k$models_test)  # one model per fold

}
\seealso{
\code{\link[randomForest:randomForest]{randomForest::randomForest()}}, \code{\link[stats:lm]{stats::lm()}}, \code{\link[stats:cor]{stats::cor()}}
}
